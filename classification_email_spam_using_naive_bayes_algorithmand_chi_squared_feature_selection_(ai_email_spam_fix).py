# -*- coding: utf-8 -*-
"""Classification Email Spam using Naive Bayes Algorithmand Chi-Squared Feature Selection (AI_Email_Spam_FIX).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HkeInuWgr5Djgax5fpbove8Zyt1XP-mF

# Classification Email Spam using Naive Bayes Algorithmand Chi-Squared Feature Selection

Dataset: https://www.kaggle.com/datasets/mfaisalqureshi/spam-email
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import accuracy_score
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

# Load data
data = pd.read_csv("/content/spam.csv")

# Preprocessing
def preprocess_text(text):
    # Remove special characters and numbers
    text = re.sub(r'[^a-zA-Z]', ' ', text)

    # Tokenize text
    tokens = word_tokenize(text)

    # Case folding (convert to lowercase)
    tokens = [token.lower() for token in tokens]

    # Stopword filtering
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(token) for token in tokens]

    # Join tokens back into text
    processed_text = ' '.join(tokens)
    return processed_text

# Menampilkan beberapa baris pertama dari dataset
print(data.head())

# Download the 'punkt' resource
nltk.download('punkt')

import nltk
nltk.download('stopwords')

# Example usage
text = " Do you want a new video handset? 750 anytime any network mins? Half Price Line Rental? Camcorder? Reply or call 08000930705 for delivery tomorrow"
processed_text = preprocess_text(text)
tokens = word_tokenize(processed_text)

print(tokens)

def case_folding(text):
    # Convert text to lowercase
    folded_text = text.lower()
    return folded_text

# Example usage
text = "Do you want a new video handset? 750 anytime any network mins? Half Price Line Rental? Camcorder? Reply or call 08000930705 for delivery tomorrow"
folded_text = case_folding(text)

print(folded_text)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def stopword_filtering(text):
    # Tokenize text
    tokens = word_tokenize(text)

    # Define stopwords
    stop_words = set(stopwords.words('english'))

    # Filter out stopwords
    filtered_tokens = [token for token in tokens if token not in stop_words]

    return filtered_tokens

# Example usage
text = "do you want a new video handset? 750 anytime any network mins? half price line rental? camcorder? reply or call 08000930705 for delivery tomorrow"
filtered_tokens = stopword_filtering(text)

print(filtered_tokens)

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

def stemming(text):
    # Tokenize text
    tokens = word_tokenize(text)

    # Initialize PorterStemmer
    stemmer = PorterStemmer()

    # Perform stemming on each token
    stemmed_tokens = [stemmer.stem(token) for token in tokens]

    return stemmed_tokens

# Example usage
text = "'want', 'new', 'video', 'handset', '?', '750', 'anytime', 'network', 'mins', '?', 'half', 'price', 'line', 'rental', '?', 'camcorder', '?', 'reply', 'call', '08000930705', 'delivery', 'tomorrow'"
stemmed_tokens = stemming(text)

print(stemmed_tokens)

# Preprocessing on data
data['processed_text'] = data['Message'].apply(preprocess_text)

# Display the results
for i in range(len(data)):
    print("Teks Awal: ", data['Message'][i])
    print("Hasil Preprocessing: ", data['processed_text'][i])
    print()

# Split data menjadi data latih dan data uji
train_data, test_data, train_labels, test_labels = train_test_split(data['Message'], data['Category'], test_size=0.2, random_state=42)

# Ekstraksi fitur menggunakan CountVectorizer pada data pelatihan dan pengujian
vectorizer = CountVectorizer()
X_train_counts = vectorizer.fit_transform(train_data)
X_test_counts = vectorizer.transform(test_data)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Melakukan PCA pada data pelatihan
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_counts.toarray())

# Memvisualisasikan data pelatihan
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1])
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Visualization of Training Data')
plt.show()

# Melakukan PCA pada data pengujian
X_test_pca = pca.transform(X_test_counts.toarray())

# Memvisualisasikan data pengujian
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1])
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Visualization of Test Data')
plt.show()

# Seleksi fitur dengan Chi-Squared pada data pelatihan
ch2 = SelectKBest(chi2, k=2500)
X_train_selected = ch2.fit_transform(X_train_counts, train_labels)
X_test_selected = ch2.transform(X_test_counts)

import numpy as np
import matplotlib.pyplot as plt

# Mengambil skor Chi-Squared
scores = ch2.scores_

# Mengurutkan skor dan indeks fitur
sorted_indices = np.argsort(scores)[::-1]
sorted_scores = np.sort(scores)[::-1]

# Memvisualisasikan skor Chi-Squared
plt.figure(figsize=(10, 6))
plt.bar(range(len(sorted_scores)), sorted_scores)
plt.xlabel('Feature Index')
plt.ylabel('Chi-Squared Score')
plt.title('Chi-Squared Scores of Features')
plt.show()

import numpy as np
from scipy.stats import chi2_contingency
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels)

# Menghitung tabel kontingensi
observed = np.zeros((X_train_selected.shape[1], len(label_encoder.classes_)))
for i, feature in enumerate(X_train_selected.T.toarray()):
    for j, label in enumerate(train_labels_encoded):
        if feature[j] > 0:
            observed[i, label] += 1

# Menghitung nilai chi-square dan p-value
chi2, p = chi2_contingency(observed)[:2]

# Menampilkan hasil seleksi fitur
selected_indices = np.argsort(chi2)[-2500:]
selected_features = [X_train_selected[index] for index in selected_indices]
print("Fitur terpilih:", selected_features)

# Menampilkan rumus perhitungan chi-square
print("\nRumus Perhitungan Chi-Square:")
print("Chi-square = ((Observed - Expected)^2) / Expected")
print("dengan:")
print("Observed: Tabel kontingensi (jumlah pengamatan pada setiap sel)")
print("Expected: Nilai yang diharapkan berdasarkan asumsi nol hipotesis")

import numpy as np
from scipy.stats import chi2_contingency
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels)

# Menghitung tabel kontingensi
observed = np.zeros((X_train_selected.shape[1], len(label_encoder.classes_)))
for i, feature in enumerate(X_train_selected.T.toarray()):
    for j, label in enumerate(train_labels_encoded):
        if feature[j] > 0:
            observed[i, label] += 1

# Menghitung nilai chi-square dan p-value
chi2, p = chi2_contingency(observed)[:2]

# Menampilkan nilai observed
print("Nilai Observed:")
print(observed)

# Menampilkan nilai expected
print("\nNilai Expected:")
print(expected)

# Menampilkan rumus perhitungan chi-square
print("\nRumus Perhitungan Chi-Square:")
print("Chi-square = ((Observed - Expected)^2) / Expected")
print("dengan:")
print("Observed: Tabel kontingensi (jumlah pengamatan pada setiap sel)")
print("Expected: Nilai yang diharapkan berdasarkan asumsi nol hipotesis")

import numpy as np
from scipy.stats import chi2_contingency
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels)

# Menghitung tabel kontingensi
observed = np.zeros((X_train_selected.shape[1], len(label_encoder.classes_)))
for i, feature in enumerate(X_train_selected.T.toarray()):
    for j, label in enumerate(train_labels_encoded):
        if feature[j] > 0:
            observed[i, label] += 1

# Menghitung nilai chi-square dan p-value
chi2, p = chi2_contingency(observed)[:2]

# Menampilkan hasil seleksi fitur
selected_indices = np.argsort(chi2)[-2500:]
selected_features = [X_test_selected[index] for index in selected_indices]
print("Fitur terpilih:", selected_features)

# Menampilkan rumus perhitungan chi-square
print("\nRumus Perhitungan Chi-Square:")
print("Chi-square = ((Observed - Expected)^2) / Expected")
print("dengan:")
print("Observed: Tabel kontingensi (jumlah pengamatan pada setiap sel)")
print("Expected: Nilai yang diharapkan berdasarkan asumsi nol hipotesis")

# Model training dengan Naive Bayes
model = MultinomialNB()
model.fit(X_train_selected, train_labels)

# Evaluasi model pada data pengujian
y_pred = model.predict(X_test_selected)

# Menghitung metrik evaluasi
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(test_labels, y_pred)
precision = precision_score(test_labels, y_pred, average='weighted')
recall = recall_score(test_labels, y_pred, average='weighted')
f1 = f1_score(test_labels, y_pred, average='weighted')

print("Akurasi: {:.2f}%".format(accuracy * 100))
print("Presisi: {:.2f}%".format(precision * 100))
print("Recall: {:.2f}%".format(recall * 100))
print("F1-score: {:.2f}%".format(f1 * 100))

import matplotlib.pyplot as plt

# Menghitung jumlah data spam dan bukan spam
spam_count = len(data[data['Category'] == 'spam'])
ham_count = len(data[data['Category'] == 'ham'])

# Memvisualisasikan jumlah spam dan bukan spam
labels = ['Spam', 'Not Spam']
sizes = [spam_count, ham_count]
colors = ['red', 'blue']

plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
plt.axis('equal')
plt.title('Proportion of Spam vs Not Spam')
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Filter data dengan kategori spam
spam_data = data[data['Category'] == 'spam']

# Menggabungkan teks dari data spam menjadi satu string
spam_text = ' '.join(spam_data['Message'])

# Membuat objek WordCloud
wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(spam_text)

# Menampilkan wordcloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Tampilkan Confusion Matrix
cm = confusion_matrix(test_labels, y_pred)
classes = ['Non-Spam', 'Spam']

plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

thresh = cm.max() / 2.0
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)

plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, roc_curve, auc

# Konversi nilai target menjadi format biner
test_labels_bin = test_labels.map({'ham': 0, 'spam': 1})

# Menghitung probabilitas prediksi spam
y_scores = model.predict_proba(X_test_selected)[:, 1]

# Menghitung precision, recall, dan threshold untuk kurva Precision-Recall
precision, recall, thresholds_pr = precision_recall_curve(test_labels_bin, y_scores)

# Menghitung nilai AUC untuk kurva Precision-Recall
auc_pr = auc(recall, precision)

# Menampilkan kurva Precision-Recall
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label='Precision-Recall Curve (AUC = {:.2f})'.format(auc_pr))
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')
plt.grid(True)
plt.show()

# Menghitung false positive rate, true positive rate, dan threshold untuk kurva ROC
fpr, tpr, thresholds_roc = roc_curve(test_labels_bin, y_scores)

# Menghitung nilai AUC untuk kurva ROC
auc_roc = auc(fpr, tpr)

# Menampilkan kurva ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(auc_roc))
plt.plot([0, 1], [0, 1], 'k--')  # Garis diagonal acak
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""ROC Curve menggambarkan hubungan antara True Positive Rate (TPR) dan False Positive Rate (FPR)"""

import matplotlib.pyplot as plt

# Akurasi
akurasi_penelitian = 98.83
akurasi_jurnal = 98.66

# Plot grafik batang dengan tampilan berbeda
plt.figure(figsize=(8, 6))
bars = plt.bar(['Penelitian', 'Jurnal'], [akurasi_penelitian, akurasi_jurnal])
plt.ylim([0, 100])
plt.xlabel('Metode')
plt.ylabel('Akurasi')
plt.title('Komparasi Akurasi dengan Jurnal Sebelumnya')

# Tampilkan nilai akurasi pada grafik
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}%', ha='center', va='bottom')

plt.show()

import matplotlib.pyplot as plt

# Akurasi
akurasi_penelitian = 98.83
akurasi_jurnal = 98.66

# Plot grafik batang dengan tampilan yang memperjelas perbedaan
plt.figure(figsize=(8, 6))
bars = plt.bar(['Penelitian', 'Jurnal'], [akurasi_penelitian, akurasi_jurnal])
plt.ylim([98.5, 99])
plt.xlabel('Metode')
plt.ylabel('Akurasi')
plt.title('Komparasi Akurasi dengan Jurnal Sebelumnya')

# Tampilkan nilai akurasi pada grafik
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}%', ha='center', va='bottom')

plt.show()